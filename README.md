# FoundationModelTutorial
A outline and set of links for beginner to learn about current Foundation Models

## Transformers and Attention

The **[annotated-transformer](https://github.com/harvardnlp/annotated-transformer/tree/master)** by Harvard.

### Task

Go through the [codes](https://github.com/harvardnlp/annotated-transformer/blob/master/AnnotatedTransformer.ipynb) and understand how Attention and transformers work. Also getting a basic grasp on what is encoder and what is decoder

## Scaling Law

## LLM

Knowing the famous branchs of Large Language Models(LLM)

[GPT3]()

[LLaMA](https://ar5iv.labs.arxiv.org/html/2302.13971v1) by Meta

LLaMA used many classic designs. and is released to be one of the currently major branches in reproducing ChatGPT.

### Task

Understanding each module's technique selected by LLaMA, and Taking a note about the data it used.

## Instruction Model

### Classic Instruction

[InstructGPT/TextDavinci_002~003](TODO)

### Chat Instruction

Understand that classic approachs beginning with ChatGPT

[ChatGPT](TODO)

[Alpaca](https://github.com/tatsu-lab/stanford_alpaca/) by stanford






# Foundation Model Tutorial
A outline and set of links for beginner to learn about current Foundation Models(FM)

## Transformers and Attention

Attention machanism serve as the foundamental building block of modern FM. Though in many cases the variation instead of the original degisn is used, but the key idea and main flow remains the same.

### Material

The **[annotated-transformer](https://github.com/harvardnlp/annotated-transformer/tree/master)** by Harvard.

### Task

Go through the [codes](https://github.com/harvardnlp/annotated-transformer/blob/master/AnnotatedTransformer.ipynb) and understand how Attention and transformers work. Also getting a basic grasp on what is encoder and what is decoder

## Scaling Law

## LLM

Knowing the famous branchs of Large Language Models(LLM)

### [GPT3]()

### [LLaMA](https://ar5iv.labs.arxiv.org/html/2302.13971v1) by Meta

LLaMA used many classic designs. and is released to be one of the currently major branches in reproducing ChatGPT.

#### Matrial

See the [paper](https://ar5iv.labs.arxiv.org/html/2302.13971v1) for LLaMA

#### Task

Understanding each module's technique selected by LLaMA, and Taking a note about the data it used.

### (Optional) Survey

#### Material

Surveys about FM/Large Model, e.g. https://arxiv.org/pdf/2302.09419.pdf

#### Task

Compare your understanding with others. understand ways of categorizing FM and list down the differences between different FM.

## Instruction Model

### Classic Instruction

[InstructGPT/TextDavinci_002~003](TODO)

### Chat Instruction

Understand that classic approachs beginning with ChatGPT

[ChatGPT](TODO)

[Alpaca](https://github.com/tatsu-lab/stanford_alpaca/) by stanford





